{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './images/input.jpg'\n",
    "URL = 'https://upload.wikimedia.org/wikipedia/commons/thumb/7/7e/%D0%9F%D0%BE%D1%80%D1%82%D1%80%D0%B5%D1%82%D0%BD%D0%BE%D0%B5_%D1%84%D0%BE%D1%82%D0%BE_%D0%91.%D0%94.%D0%9C%D0%B5%D0%BD%D0%B4%D0%B5%D0%BB%D0%B5%D0%B2%D0%B8%D1%87%D0%B0.jpg/400px-%D0%9F%D0%BE%D1%80%D1%82%D1%80%D0%B5%D1%82%D0%BD%D0%BE%D0%B5_%D1%84%D0%BE%D1%82%D0%BE_%D0%91.%D0%94.%D0%9C%D0%B5%D0%BD%D0%B4%D0%B5%D0%BB%D0%B5%D0%B2%D0%B8%D1%87%D0%B0.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from torchvision import transforms\n",
    "from warnings import warn\n",
    "from torchvision.models.vgg import VGG, cfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.bin', 'rb') as fin:\n",
    "    vocab = pickle.load(fin)\n",
    "with open('word_to_index.bin', 'rb') as fin:\n",
    "    word_to_index = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_ix = word_to_index['#END#']\n",
    "unk_ix = word_to_index['#UNK#']\n",
    "pad_ix = word_to_index['#PAD#']\n",
    "\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences), max_len), dtype='int32') + pad_ix\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [word_to_index.get(word, unk_ix) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductScore(nn.Module):\n",
    "    \"\"\"\n",
    "    Vaswani et al. \"Attention Is All You Need\", 2017.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, queries, keys):\n",
    "        \"\"\"\n",
    "        queries:  [batch_size x num_queries x dim]\n",
    "        keys:     [batch_size x num_objects x dim]\n",
    "        Returns a tensor of scores with shape [batch_size x num_queries x num_objects].\n",
    "        \"\"\"\n",
    "        result = torch.bmm(queries, keys.permute(0, 2, 1)) / (keys.shape[2]**0.5)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, scorer):\n",
    "        super().__init__()\n",
    "        self.scorer = scorer\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"\n",
    "        queries:         [batch_size x num_queries x query_feature_dim]\n",
    "        keys:            [batch_size x num_objects x key_feature_dim]\n",
    "        values:          [batch_size x num_objects x obj_feature_dim]\n",
    "        Returns matrix of responses for queries with shape [batch_size x num_queries x obj_feature_dim].\n",
    "        Saves detached weights as self.attention_map.\n",
    "        \"\"\"\n",
    "        scores = self.scorer(queries, keys)\n",
    "        weights = F.softmax(scores, dim=2) \n",
    "        self.attention_map = weights.detach()\n",
    "        result = torch.bmm(weights, values)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionNet(nn.Module):\n",
    "    def __init__(self, n_tokens=0, emb_size=128, lstm_units=256, cnn_channels=512):\n",
    "        \"\"\" A recurrent 'head' network for image captioning. Read scheme below. \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        # a layer that converts conv features to \n",
    "        self.cnn_to_h0 = nn.Linear(cnn_channels, lstm_units)\n",
    "        self.cnn_to_c0 = nn.Linear(cnn_channels, lstm_units)\n",
    "        \n",
    "        # recurrent part, please create the layers as per scheme above.\n",
    "\n",
    "        # create embedding for input words. Use the parameters (e.g. emb_size).\n",
    "        self.emb = nn.Embedding(n_tokens, emb_size, max_norm=5)\n",
    "            \n",
    "        # attention: create attention over image spatial positions\n",
    "        # The query is previous lstm hidden state, the keys are transformed cnn features,\n",
    "        # the values are cnn features\n",
    "        self.attention = Attention(ScaledDotProductScore())\n",
    "        \n",
    "        # attention: create transform from cnn features to the keys\n",
    "        # Hint: one linear layer shoud work\n",
    "        # Hint: the dimensionality of keys should be lstm_units as lstm\n",
    "        #       hidden state is the attention query\n",
    "        self.cnn_to_attn_key = nn.Linear(cnn_channels, lstm_units)\n",
    "                \n",
    "        # lstm: create a recurrent core of your network. Use LSTMCell\n",
    "        self.lstm = nn.LSTMCell(cnn_channels+emb_size, lstm_units)\n",
    "\n",
    "        # create logits: MLP that takes attention response, lstm hidden state\n",
    "        # and the previous word embedding as an input and computes one number per token\n",
    "        # Hint: I used an architecture with one hidden layer, but you may try deeper ones\n",
    "        self.logits_mlp = nn.Linear(cnn_channels+emb_size+lstm_units, n_tokens)\n",
    "        \n",
    "    def forward(self, image_features, captions_ix):\n",
    "        \"\"\" \n",
    "        Apply the network in training mode. \n",
    "        :param image_features: torch tensor containing VGG features for each position.\n",
    "                               shape: [batch, cnn_channels, width * height]\n",
    "        :param captions_ix: torch tensor containing captions as matrix. shape: [batch, word_i]. \n",
    "            padded with pad_ix\n",
    "        :returns: logits for next token at each tick, shape: [batch, word_i, n_tokens]\n",
    "        \"\"\"\n",
    "        initial_cell = self.cnn_to_c0(image_features.mean(2))\n",
    "        initial_hid = self.cnn_to_h0(image_features.mean(2))\n",
    "        \n",
    "        image_features = image_features.transpose(1, 2)\n",
    "        \n",
    "        # compute embeddings for captions_ix\n",
    "        captions_emb = self.emb(captions_ix)\n",
    "        \n",
    "        reccurent_out = []\n",
    "        attention_map = []\n",
    "        \n",
    "        cell = initial_cell\n",
    "        hid = initial_hid\n",
    "        key = self.cnn_to_attn_key(image_features)\n",
    "        for i in range(captions_ix.shape[1]):\n",
    "            a = self.attention(hid.view(len(key), 1, -1), key, image_features).view(len(key), -1)\n",
    "            attention_map.append(self.attention.attention_map)\n",
    "            hid, cell = self.lstm(torch.cat((captions_emb[:, i], a), dim=1), (hid, cell))\n",
    "            reccurent_out.append(torch.cat((hid, a, captions_emb[:, i]), dim = 1).view(len(key), 1, -1))\n",
    "        reccurent_out = torch.cat(reccurent_out, dim=1)\n",
    "        attention_map = torch.cat(attention_map, dim=1)\n",
    "        \n",
    "        # compute logits for next token probabilities\n",
    "        # based on the stored in (2.7) values (reccurent_out)\n",
    "        bs, cl, dim = reccurent_out.shape\n",
    "        logits = self.logits_mlp(reccurent_out.view(-1, dim)).view(bs, cl, -1)\n",
    "        \n",
    "        # return logits and attention maps from (2.4)\n",
    "        return logits, attention_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeheadedVGG19(VGG):\n",
    "    \"\"\" Like torchvision.models.inception.Inception3 but the head goes separately \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_for_attn = x= self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = x = self.classifier(x)\n",
    "        return x_for_attn, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('network.bin', 'rb') as fin:\n",
    "    network = pickle.load(fin)\n",
    "with open('features_net.bin', 'rb') as fin:\n",
    "    features_net = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image, caption_prefix = (\"#START#\",), \n",
    "                     t=1, sample=True, max_len=100):\n",
    "    \n",
    "    assert isinstance(image, np.ndarray) and np.max(image) <= 1\\\n",
    "           and np.min(image) >=0 and image.shape[-1] == 3\n",
    "    \n",
    "    image = torch.tensor(image.transpose([2, 0, 1]), dtype=torch.float32)\n",
    "    \n",
    "    vectors_9x9, logits = features_net(image[None])\n",
    "    caption_prefix = list(caption_prefix)\n",
    "    \n",
    "    attention_maps = []\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        \n",
    "        prefix_ix = as_matrix([caption_prefix])\n",
    "        prefix_ix = torch.tensor(prefix_ix, dtype=torch.int64)\n",
    "        input_features = vectors_9x9.view(vectors_9x9.shape[0], vectors_9x9.shape[1], -1)\n",
    "        if next(network.parameters()).is_cuda:\n",
    "            input_features, prefix_ix = input_features.cuda(), prefix_ix.cuda()\n",
    "        else:\n",
    "            input_features, prefix_ix = input_features.cpu(), prefix_ix.cpu()\n",
    "        next_word_logits, cur_attention_map = network(input_features, prefix_ix)\n",
    "        next_word_logits = next_word_logits[0, -1]\n",
    "        cur_attention_map = cur_attention_map[0, -1]\n",
    "        next_word_probs = F.softmax(next_word_logits, -1).detach().cpu().numpy()\n",
    "        attention_maps.append(cur_attention_map.detach().cpu())\n",
    "        \n",
    "        assert len(next_word_probs.shape) ==1, 'probs must be one-dimensional'\n",
    "        next_word_probs = next_word_probs ** t / np.sum(next_word_probs ** t) # apply temperature\n",
    "\n",
    "        if sample:\n",
    "            next_word = np.random.choice(vocab, p=next_word_probs) \n",
    "        else:\n",
    "            next_word = vocab[np.argmax(next_word_probs)]\n",
    "\n",
    "        caption_prefix.append(next_word)\n",
    "\n",
    "        if next_word==\"#END#\":\n",
    "            break\n",
    "\n",
    "    return caption_prefix, attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from file\n",
    "img = default_loader(path)\n",
    "\n",
    "# from URL\n",
    "response = requests.get(URL)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "img = np.array(img)\n",
    "img = img/img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n"
     ]
    }
   ],
   "source": [
    "result = ' '.join(generate_caption(img, t=5)[0][1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man wearing a tie with a tie .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = ''.join(result)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
